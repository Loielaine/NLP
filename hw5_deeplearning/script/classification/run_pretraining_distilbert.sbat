#!/bin/bash
# "#SBATCH" directives that convey submission options:

###### The name of the job
#SBATCH --job-name=pretraining_distilbert

###### When to send e-mail: pick from NONE, BEGIN, END, FAIL, REQUEUE, ALL
#SBATCH --mail-user=liyixi@umich.edu
#SBATCH --mail-type=NONE

###### Resources for your job
###### number of physical nodes
#SBATCH --partition=gpu
#SBATCH --gres=gpu:1
#SBATCH --nodes=1

###### number of task per node (number of CPU-cores per node)
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1

###### Memory per CPU-core
#SBATCH --mem-per-cpu=16gb

###### Maximum amount of time the job will be allowed to run
###### Recommended formats: MM:SS, HH:MM:SS, DD-HH:MM
#SBATCH --time=02:00:00

###### The resource account; who pays
#SBATCH --output=/home/%u/hw5/output/fine-tuning/%x-%j.log

source activate test_env
export http_proxy="http://proxy.arc-ts.umich.edu:3128/"
export https_proxy="http://proxy.arc-ts.umich.edu:3128/"
export ftp_proxy="http://proxy.arc-ts.umich.edu:3128/"
export no_proxy="localhost,127.0.0.1,.localdomain,.umich.edu"
export HTTP_PROXY="${http_proxy}"
export HTTPS_PROXY="${https_proxy}"
export FTP_PROXY="${ftp_proxy}"
export NO_PROXY="${no_proxy}"

python model_pretraining_auto.py --task_name="pretraining_distilbert" --train_data_file="/home/liyixi/hw5/data/generation/train.txt" "--output_dir=/home/liyixi/hw5/output/fine-tuning/" --model_type="distilbert-base-uncased" --model_name_or_path="distilbert-base-uncased" --eval_data_file="/home/liyixi/hw5/data/generation/dev.txt" --do_train --do_eval --save_steps=5000 --overwrite_output_dir --mlm --line_by_line


