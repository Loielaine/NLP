{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try out some toy data\n",
    "\n",
    "Consider a village where all villagers are either healthy or have a fever and only the village doctor can determine whether each has a fever. The doctor diagnoses fever by asking patients how they feel. The villagers may only answer that they feel normal, dizzy, or cold.\n",
    "\n",
    "The doctor believes that the health condition of his patients operate as a discrete Markov chain. There are two states, \"Healthy\" and \"Fever\", but the doctor cannot observe them directly; they are hidden from him. On each day, there is a certain chance that the patient will tell the doctor he/she is \"normal\", \"cold\", or \"dizzy\", depending on their health condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the sequence of observations (analogous to the words in a sentence)\n",
    "obs = ['normal', 'cold', 'dizzy']\n",
    "\n",
    "# This is the set of hidden states that emit observations \n",
    "# (analogous to Parts of Speech, Semantic Roles, Semantic Categories, etc., in NLP)\n",
    "states = ('Healthy', 'Fever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this piece of code, `start_p` (`p` = probability) represents the doctor's belief about which state the HMM is in when the patient first visits (all he knows is that the patient tends to be healthy). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately `{'Healthy': 0.57, 'Fever': 0.43}`. \n",
    "\n",
    "The `transition_p` represents the change of the health condition in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow the patient will have a fever if he is healthy today.\n",
    "\n",
    "The `emission_p` represents how likely the patient is to feel on each day. If he is healthy, there is a 50% chance that he feels normal; if he has a fever, there is a 60% chance that he feels dizzy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the probility of starting the sequence in each state\n",
    "start_p = {\n",
    "    'Healthy': 0.6, \n",
    "    'Fever': 0.4\n",
    "}\n",
    "\n",
    "# This is the probability of transitioning between states at each time step\n",
    "trans_p = {\n",
    "   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},\n",
    "   'Fever' : {'Healthy': 0.4, 'Fever': 0.6}\n",
    "   }\n",
    "\n",
    "# This is the probability of each state emitting each observation\n",
    "emit_p = {\n",
    "   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    "   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can visualize these start, transition, and emission probabilities graphically\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/An_example_of_HMM.png\" width=400>\n",
    "Image credit: By Reelsun - By using open office draw, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=19118190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's answer our original question using the Viterbi algorithm!\n",
    "\n",
    "The function viterbi takes the following arguments: obs is the sequence of observations, e.g. `['normal', 'cold', 'dizzy']`; states is the set of hidden states; `start_p` is the start probability; `trans_p` are the transition probabilities; and `emit_p` are the emission probabilities. For simplicity of code, we assume that the observation sequence obs is non-empty and that `trans_p[i][j]` and `emit_p[i][j]` is defined for all states i,j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"] * trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    for line in dptable(V):\n",
    "        print(line)\n",
    "    opt = []\n",
    "    # The highest probability\n",
    "    max_prob = max(value[\"prob\"] for value in V[-1].values())\n",
    "    previous = None\n",
    "    # Get most probable state and its backtrack\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] == max_prob:\n",
    "            opt.append(st)\n",
    "            previous = st\n",
    "            break\n",
    "    # Follow the backtrack till the first observation\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    " \n",
    "    print('The steps of states are ' + ' '.join(opt) + ' with highest probability of %s' % max_prob)\n",
    "\n",
    "def dptable(V):\n",
    "     # Print a table of steps from dictionary\n",
    "     yield \" \".join((\"%12d\" % i) for i in range(len(V)))\n",
    "     for state in V[0]:\n",
    "         yield \"%.7s: \" % state + \" \".join(\"%.7s\" % (\"%f\" % v[state][\"prob\"]) for v in V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0            1            2\n",
      "Healthy: 0.30000 0.08400 0.00588\n",
      "Fever: 0.04000 0.02700 0.01512\n",
      "The steps of states are Healthy Healthy Fever with highest probability of 0.01512\n"
     ]
    }
   ],
   "source": [
    "viterbi(obs, states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try building a POS tagger!\n",
    "\n",
    "We've included a file `pos-training-data.tsv` that contains POS-tagged data from Twitter.  The file is formatted as two columns where column 1 is the word and column 2 is the part of speech.  Sentences are separated by a blank line, which you'll need to take into account when computing the starting transition probabilities.\n",
    "\n",
    "Your job is to read the data and then fill the `states`, `start_p`, `trans_p`, and `emit_p` data structures based on the observed word and POS transitions.  Once those are done, you can run the viterbi algorithm as-is!  We recommend trying this process in two steps\n",
    "1. First, *count* the frequencies for the start, transition, and emission (Hint: `defaultdict` and `Counter` are handy here!)\n",
    "2. Then, *normalize* the counts to be probability distributions (e.g., convert `start_counts` to `start_p`)\n",
    "\n",
    "Side note:  The date comes from the paper <a href=\"http://www.ark.cs.cmu.edu/TweetNLP/gimpel+etal.acl11.pdf \">Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a> by Kevin Gimpel, Nathan Schneider, Brendan O'Connor, Dipanjan Das, Daniel Mills,   Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and  Noah A. Smith, iun the Proceedings of ACL 2011.   Also, here's their project's <a href=\"https://github.com/brendano/ark-tweet-nlp/\">website</a> and the <a href=\"http://www.cs.cmu.edu/~ark/TweetNLP/annot_guidelines.pdf\">part of speech guidelines</a> (you don't really need to read these)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "with open('pos-training-data.tsv', encoding='utf-8') as f:\n",
    "       \n",
    "    for line_no, line in enumerate(f):\n",
    "\n",
    "        # Do your counting thing here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now normalize the counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test it out!\n",
    "Once you've finished your code, try it out here states, start_p, trans_p, emit_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "viterbi(['I', 'like', 'monkeys'], states, start_p, trans_p, emit_p)\n",
    "viterbi(['Just', 'finished', 'my', 'morning', 'Zumba', 'class'], states, start_p, trans_p, emit_p)\n",
    "viterbi(['I', 'am', 'so', 'bored', 'right', 'now'], states, start_p, trans_p, emit_p)\n",
    "viterbi(['I', 'luv', 'youuuuuu'], states, start_p, trans_p, emit_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have even more time?\n",
    "We've also included `pos-test-data.tsv`.  For fun, try loading these instances and see how well your Viterbi-based POS tagger works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
